{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib as plt\n",
    "import logging\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST dataset, convert to binary pixel values\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Lambda(lambda x: torch.where(x > 0,1,0))\n",
    "                             ]))\n",
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=False)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Lambda(lambda x: torch.where(x > 0,1,0))\n",
    "                             ]))\n",
    "testloader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "# set device to run\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model  \n",
    "class TransformationNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformationNet, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(input_dim, 64, 1)\n",
    "        self.conv_2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv_3 = nn.Conv1d(128, 256, 1)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "        self.bn_2 = nn.BatchNorm1d(128)\n",
    "        self.bn_3 = nn.BatchNorm1d(256)\n",
    "        self.bn_4 = nn.BatchNorm1d(256)\n",
    "        self.bn_5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, self.output_dim*self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_points = x.shape[1]\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "\n",
    "        x = nn.MaxPool1d(num_points)(x)\n",
    "        x = x.view(-1, 256)\n",
    "\n",
    "        x = F.relu(self.bn_4(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_5(self.fc_2(x)))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        identity_matrix = torch.eye(self.output_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            identity_matrix = identity_matrix.cuda()\n",
    "        x = x.view(-1, self.output_dim, self.output_dim) + identity_matrix\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasePointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, point_dimension):\n",
    "        super(BasePointNet, self).__init__()\n",
    "        self.input_transform = TransformationNet(input_dim=point_dimension, output_dim=point_dimension)\n",
    "        self.feature_transform = TransformationNet(input_dim=64, output_dim=64)\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(point_dimension, 64, 1)\n",
    "        self.conv_2 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv_3 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv_4 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv_5 = nn.Conv1d(128, 256, 1)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "        self.bn_2 = nn.BatchNorm1d(64)\n",
    "        self.bn_3 = nn.BatchNorm1d(64)\n",
    "        self.bn_4 = nn.BatchNorm1d(128)\n",
    "        self.bn_5 = nn.BatchNorm1d(256)\n",
    "        \n",
    "\n",
    "    def forward(self, x, plot=False):\n",
    "        num_points = x.shape[1]\n",
    "        \n",
    "        input_transform = self.input_transform(x) # T-Net tensor [batch, 3, 3]\n",
    "        x = torch.bmm(x, input_transform) # Batch matrix-matrix product \n",
    "        x = x.transpose(2, 1) \n",
    "        tnet_out=x.cpu().detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "\n",
    "        feature_transform = self.feature_transform(x) # T-Net tensor [batch, 64, 64]\n",
    "        x = torch.bmm(x, feature_transform)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "        x = F.relu(self.bn_4(self.conv_4(x)))\n",
    "        x = F.relu(self.bn_5(self.conv_5(x)))\n",
    "        # x, ix = nn.MaxPool1d(num_points, return_indices=True)(x)  # max-pooling\n",
    "        # x = x.view(-1, 256)  # global feature vector \n",
    "\n",
    "        return x, feature_transform, tnet_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform image to 3D (x, y, binary value)\n",
    "def img_to_3d(img):\n",
    "    # get coordinates of pixels\n",
    "    coords_x, coords_y = torch.meshgrid(torch.arange(0, img.size(1)), torch.arange(0, img.size(2)))\n",
    "    coords_x = coords_x.flatten().float().unsqueeze(1)\n",
    "    coords_y = coords_y.flatten().float().unsqueeze(1)\n",
    "    values = img.view(-1).unsqueeze(1)\n",
    "    pc = torch.cat((coords_x, coords_y, values), dim=1)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1034.290161\n",
      "Epoch [1/10], Loss: 819.665649\n",
      "Epoch [1/10], Loss: 766.967407\n",
      "Epoch [1/10], Loss: 697.906860\n",
      "Epoch [1/10], Loss: 722.931702\n",
      "Epoch [1/10], Loss: 729.396973\n",
      "Epoch [1/10], Loss: 671.736511\n",
      "Epoch [1/10], Loss: 684.474670\n",
      "Epoch [1/10], Loss: 709.041260\n",
      "Epoch [1/10], Loss: 658.612671\n",
      "Epoch [1/10], Loss: 706.310486\n",
      "Epoch [1/10], Loss: 667.150085\n",
      "Epoch [1/10], Loss: 693.060242\n",
      "Epoch [1/10], Loss: 678.687866\n",
      "Epoch [1/10], Loss: 642.552734\n",
      "Epoch [1/10], Loss: 619.414551\n",
      "Epoch [1/10], Loss: 643.144897\n",
      "Epoch [1/10], Loss: 647.601501\n",
      "Epoch [1/10], Loss: 677.268921\n",
      "Epoch [1/10], Loss: 655.306091\n",
      "Epoch [1/10], Loss: 660.429077\n",
      "Epoch [1/10], Loss: 704.577942\n",
      "Epoch [1/10], Loss: 640.990906\n",
      "Epoch [1/10], Loss: 648.795532\n",
      "Epoch [1/10], Loss: 688.607788\n",
      "Epoch [1/10], Loss: 648.267212\n",
      "Epoch [1/10], Loss: 649.314941\n",
      "Epoch [1/10], Loss: 662.838623\n",
      "Epoch [1/10], Loss: 629.916504\n",
      "Epoch [1/10], Loss: 657.596619\n",
      "Epoch [1/10], Loss: 651.961792\n",
      "Epoch [1/10], Loss: 681.706665\n",
      "Epoch [1/10], Loss: 678.252625\n",
      "Epoch [1/10], Loss: 624.860779\n",
      "Epoch [1/10], Loss: 633.037415\n",
      "Epoch [1/10], Loss: 652.629028\n",
      "Epoch [1/10], Loss: 691.784241\n",
      "Epoch [1/10], Loss: 644.168701\n",
      "Epoch [1/10], Loss: 630.782837\n",
      "Epoch [1/10], Loss: 646.594238\n",
      "Epoch [1/10], Loss: 622.091553\n",
      "Epoch [1/10], Loss: 677.289368\n",
      "Epoch [1/10], Loss: 655.285889\n",
      "Epoch [1/10], Loss: 665.359314\n",
      "Epoch [1/10], Loss: 731.601379\n",
      "Epoch [1/10], Loss: 652.820923\n",
      "Epoch [1/10], Loss: 607.799927\n",
      "Epoch [1/10], Loss: 647.353271\n",
      "Epoch [1/10], Loss: 640.166321\n",
      "Epoch [1/10], Loss: 645.346924\n",
      "Epoch [1/10], Loss: 607.700195\n",
      "Epoch [1/10], Loss: 653.397827\n",
      "Epoch [1/10], Loss: 683.310547\n",
      "Epoch [1/10], Loss: 625.181030\n",
      "Epoch [1/10], Loss: 612.468689\n",
      "Epoch [1/10], Loss: 679.465088\n",
      "Epoch [1/10], Loss: 644.604858\n",
      "Epoch [1/10], Loss: 670.701965\n",
      "Epoch [1/10], Loss: 629.754089\n",
      "Epoch [1/10], Loss: 638.092773\n",
      "Epoch [1/10], Loss: 634.665833\n",
      "Epoch [1/10], Loss: 624.780151\n",
      "Epoch [1/10], Loss: 641.659912\n",
      "Epoch [1/10], Loss: 638.061523\n",
      "Epoch [1/10], Loss: 630.098511\n",
      "Epoch [1/10], Loss: 645.453491\n",
      "Epoch [1/10], Loss: 626.846619\n",
      "Epoch [1/10], Loss: 605.107910\n",
      "Epoch [1/10], Loss: 635.854370\n",
      "Epoch [1/10], Loss: 671.489685\n",
      "Epoch [1/10], Loss: 655.792542\n",
      "Epoch [1/10], Loss: 603.475647\n",
      "Epoch [1/10], Loss: 666.117126\n",
      "Epoch [1/10], Loss: 694.145752\n",
      "Epoch [1/10], Loss: 642.028320\n",
      "Epoch [1/10], Loss: 643.696838\n",
      "Epoch [1/10], Loss: 621.967651\n",
      "Epoch [1/10], Loss: 606.120850\n",
      "Epoch [1/10], Loss: 611.831848\n",
      "Epoch [1/10], Loss: 622.885071\n",
      "Epoch [1/10], Loss: 626.188843\n",
      "Epoch [1/10], Loss: 638.849487\n",
      "Epoch [1/10], Loss: 653.706787\n",
      "Epoch [1/10], Loss: 634.130737\n",
      "Epoch [1/10], Loss: 694.340027\n",
      "Epoch [1/10], Loss: 594.765747\n",
      "Epoch [1/10], Loss: 623.074280\n",
      "Epoch [1/10], Loss: 610.682678\n",
      "Epoch [1/10], Loss: 629.042480\n",
      "Epoch [1/10], Loss: 669.394714\n",
      "Epoch [1/10], Loss: 624.845215\n",
      "Epoch [1/10], Loss: 628.617798\n",
      "Epoch [1/10], Loss: 689.404846\n",
      "Epoch [1/10], Loss: 652.387390\n",
      "Epoch [1/10], Loss: 689.298035\n",
      "Epoch [1/10], Loss: 673.975342\n",
      "Epoch [1/10], Loss: 649.985779\n",
      "Epoch [1/10], Loss: 652.439514\n",
      "Epoch [1/10], Loss: 634.841187\n",
      "Epoch [1/10], Loss: 696.665527\n",
      "Epoch [1/10], Loss: 612.770508\n",
      "Epoch [1/10], Loss: 643.405945\n",
      "Epoch [1/10], Loss: 610.014038\n",
      "Epoch [1/10], Loss: 645.704773\n",
      "Epoch [1/10], Loss: 580.635986\n",
      "Epoch [1/10], Loss: 626.782349\n",
      "Epoch [1/10], Loss: 607.863281\n",
      "Epoch [1/10], Loss: 608.299683\n",
      "Epoch [1/10], Loss: 652.185120\n",
      "Epoch [1/10], Loss: 596.972656\n",
      "Epoch [1/10], Loss: 631.378479\n",
      "Epoch [1/10], Loss: 611.877075\n",
      "Epoch [1/10], Loss: 650.026062\n",
      "Epoch [1/10], Loss: 624.534302\n",
      "Epoch [1/10], Loss: 606.327942\n",
      "Epoch [1/10], Loss: 654.903809\n",
      "Epoch [1/10], Loss: 630.241699\n",
      "Epoch [1/10], Loss: 603.195435\n",
      "Epoch [1/10], Loss: 643.389709\n",
      "Epoch [1/10], Loss: 635.318970\n",
      "Epoch [1/10], Loss: 630.201477\n",
      "Epoch [1/10], Loss: 674.579102\n",
      "Epoch [1/10], Loss: 643.334656\n",
      "Epoch [1/10], Loss: 636.870605\n",
      "Epoch [1/10], Loss: 640.827271\n",
      "Epoch [1/10], Loss: 629.550537\n",
      "Epoch [1/10], Loss: 617.875732\n",
      "Epoch [1/10], Loss: 608.581604\n",
      "Epoch [1/10], Loss: 642.814636\n",
      "Epoch [1/10], Loss: 610.130310\n",
      "Epoch [1/10], Loss: 614.250610\n",
      "Epoch [1/10], Loss: 667.411377\n",
      "Epoch [1/10], Loss: 635.980713\n",
      "Epoch [1/10], Loss: 622.603699\n",
      "Epoch [1/10], Loss: 691.472229\n",
      "Epoch [1/10], Loss: 616.652527\n",
      "Epoch [1/10], Loss: 639.923523\n",
      "Epoch [1/10], Loss: 623.767700\n",
      "Epoch [1/10], Loss: 690.129578\n",
      "Epoch [1/10], Loss: 644.417114\n",
      "Epoch [1/10], Loss: 634.015930\n",
      "Epoch [1/10], Loss: 610.054443\n",
      "Epoch [1/10], Loss: 606.557983\n",
      "Epoch [1/10], Loss: 629.058594\n",
      "Epoch [1/10], Loss: 645.480591\n",
      "Epoch [1/10], Loss: 622.160950\n",
      "Epoch [1/10], Loss: 638.875183\n",
      "Epoch [1/10], Loss: 597.538696\n",
      "Epoch [1/10], Loss: 649.838867\n",
      "Epoch [1/10], Loss: 636.995239\n",
      "Epoch [1/10], Loss: 629.750061\n",
      "Epoch [1/10], Loss: 648.400146\n",
      "Epoch [1/10], Loss: 643.046082\n",
      "Epoch [1/10], Loss: 651.355896\n",
      "Epoch [1/10], Loss: 609.152954\n",
      "Epoch [1/10], Loss: 627.897217\n",
      "Epoch [1/10], Loss: 636.505920\n",
      "Epoch [1/10], Loss: 675.056946\n",
      "Epoch [1/10], Loss: 629.147949\n",
      "Epoch [1/10], Loss: 602.704895\n",
      "Epoch [1/10], Loss: 619.122131\n",
      "Epoch [1/10], Loss: 676.997742\n",
      "Epoch [1/10], Loss: 651.448242\n",
      "Epoch [1/10], Loss: 625.692017\n",
      "Epoch [1/10], Loss: 631.218872\n",
      "Epoch [1/10], Loss: 655.345703\n",
      "Epoch [1/10], Loss: 614.192871\n",
      "Epoch [1/10], Loss: 622.787048\n",
      "Epoch [1/10], Loss: 637.816040\n",
      "Epoch [1/10], Loss: 601.596375\n",
      "Epoch [1/10], Loss: 615.583252\n",
      "Epoch [1/10], Loss: 678.860168\n",
      "Epoch [1/10], Loss: 665.142761\n",
      "Epoch [1/10], Loss: 617.197083\n",
      "Epoch [1/10], Loss: 604.635742\n",
      "Epoch [1/10], Loss: 627.784790\n",
      "Epoch [1/10], Loss: 647.099487\n",
      "Epoch [1/10], Loss: 615.125488\n",
      "Epoch [1/10], Loss: 627.014832\n",
      "Epoch [1/10], Loss: 641.984436\n",
      "Epoch [1/10], Loss: 682.010498\n",
      "Epoch [1/10], Loss: 645.206909\n",
      "Epoch [1/10], Loss: 658.528931\n",
      "Epoch [1/10], Loss: 661.686096\n",
      "Epoch [1/10], Loss: 625.379761\n",
      "Epoch [1/10], Loss: 657.848633\n",
      "Epoch [1/10], Loss: 607.176575\n",
      "Epoch [1/10], Loss: 644.237976\n",
      "Epoch [1/10], Loss: 632.024597\n",
      "Epoch [1/10], Loss: 645.454712\n",
      "Epoch [1/10], Loss: 657.223938\n",
      "Epoch [1/10], Loss: 642.049377\n",
      "Epoch [1/10], Loss: 610.900024\n",
      "Epoch [1/10], Loss: 638.540344\n",
      "Epoch [1/10], Loss: 644.092957\n",
      "Epoch [1/10], Loss: 601.352600\n",
      "Epoch [1/10], Loss: 693.307434\n",
      "Epoch [1/10], Loss: 629.318970\n",
      "Epoch [1/10], Loss: 626.878906\n",
      "Epoch [1/10], Loss: 625.553467\n",
      "Epoch [1/10], Loss: 663.255981\n",
      "Epoch [1/10], Loss: 623.110107\n",
      "Epoch [1/10], Loss: 630.202881\n",
      "Epoch [1/10], Loss: 631.407593\n",
      "Epoch [1/10], Loss: 619.718201\n",
      "Epoch [1/10], Loss: 638.824402\n",
      "Epoch [1/10], Loss: 612.346436\n",
      "Epoch [1/10], Loss: 641.734619\n",
      "Epoch [1/10], Loss: 608.887939\n",
      "Epoch [1/10], Loss: 643.272156\n",
      "Epoch [1/10], Loss: 656.342651\n",
      "Epoch [1/10], Loss: 674.512573\n",
      "Epoch [1/10], Loss: 651.426758\n",
      "Epoch [1/10], Loss: 625.757935\n",
      "Epoch [1/10], Loss: 615.929626\n",
      "Epoch [1/10], Loss: 653.035095\n",
      "Epoch [1/10], Loss: 640.022522\n",
      "Epoch [1/10], Loss: 626.662598\n",
      "Epoch [1/10], Loss: 638.150452\n",
      "Epoch [1/10], Loss: 668.840942\n",
      "Epoch [1/10], Loss: 721.326050\n",
      "Epoch [1/10], Loss: 631.725342\n",
      "Epoch [1/10], Loss: 649.204895\n",
      "Epoch [1/10], Loss: 623.378723\n",
      "Epoch [1/10], Loss: 654.191589\n",
      "Epoch [1/10], Loss: 624.742065\n",
      "Epoch [1/10], Loss: 657.210144\n",
      "Epoch [1/10], Loss: 604.123108\n",
      "Epoch [1/10], Loss: 609.630371\n",
      "Epoch [1/10], Loss: 624.635010\n",
      "Epoch [1/10], Loss: 631.714478\n",
      "Epoch [1/10], Loss: 689.269653\n",
      "Epoch [1/10], Loss: 635.478149\n",
      "Epoch [1/10], Loss: 609.868042\n",
      "Epoch [1/10], Loss: 625.062805\n",
      "Epoch [1/10], Loss: 655.175720\n",
      "Epoch [1/10], Loss: 592.458740\n",
      "Epoch [1/10], Loss: 612.112488\n",
      "Epoch [1/10], Loss: 638.694946\n",
      "Epoch [1/10], Loss: 603.424622\n",
      "Epoch [1/10], Loss: 617.263489\n",
      "Epoch [1/10], Loss: 635.046204\n",
      "Epoch [1/10], Loss: 654.663452\n",
      "Epoch [1/10], Loss: 692.115967\n",
      "Epoch [1/10], Loss: 642.240723\n",
      "Epoch [1/10], Loss: 640.130493\n",
      "Epoch [1/10], Loss: 654.258789\n",
      "Epoch [1/10], Loss: 628.225586\n",
      "Epoch [1/10], Loss: 615.196228\n",
      "Epoch [1/10], Loss: 621.223816\n",
      "Epoch [1/10], Loss: 688.488403\n",
      "Epoch [1/10], Loss: 642.613892\n",
      "Epoch [1/10], Loss: 622.821167\n",
      "Epoch [1/10], Loss: 670.721375\n",
      "Epoch [1/10], Loss: 678.227539\n",
      "Epoch [1/10], Loss: 607.518799\n",
      "Epoch [1/10], Loss: 618.210876\n",
      "Epoch [1/10], Loss: 634.318481\n",
      "Epoch [1/10], Loss: 639.898926\n",
      "Epoch [1/10], Loss: 611.488647\n",
      "Epoch [1/10], Loss: 674.397278\n",
      "Epoch [1/10], Loss: 625.211365\n",
      "Epoch [1/10], Loss: 654.142883\n",
      "Epoch [1/10], Loss: 602.794556\n",
      "Epoch [1/10], Loss: 672.053223\n",
      "Epoch [1/10], Loss: 602.061584\n",
      "Epoch [1/10], Loss: 640.221558\n",
      "Epoch [1/10], Loss: 653.144958\n",
      "Epoch [1/10], Loss: 637.855591\n",
      "Epoch [1/10], Loss: 650.774170\n",
      "Epoch [1/10], Loss: 638.501099\n",
      "Epoch [1/10], Loss: 619.273987\n",
      "Epoch [1/10], Loss: 646.352234\n",
      "Epoch [1/10], Loss: 648.145203\n",
      "Epoch [1/10], Loss: 646.292664\n",
      "Epoch [1/10], Loss: 643.138916\n",
      "Epoch [1/10], Loss: 634.277954\n",
      "Epoch [1/10], Loss: 629.795959\n",
      "Epoch [1/10], Loss: 612.341919\n",
      "Epoch [1/10], Loss: 623.443176\n",
      "Epoch [1/10], Loss: 613.925415\n",
      "Epoch [1/10], Loss: 678.204468\n",
      "Epoch [1/10], Loss: 633.542053\n",
      "Epoch [1/10], Loss: 624.117493\n",
      "Epoch [1/10], Loss: 683.280762\n",
      "Epoch [1/10], Loss: 633.705627\n",
      "Epoch [1/10], Loss: 617.165222\n",
      "Epoch [1/10], Loss: 613.093750\n",
      "Epoch [1/10], Loss: 625.475708\n",
      "Epoch [1/10], Loss: 627.306396\n",
      "Epoch [1/10], Loss: 630.329895\n",
      "Epoch [1/10], Loss: 646.163086\n",
      "Epoch [1/10], Loss: 601.646240\n",
      "Epoch [1/10], Loss: 646.688599\n",
      "Epoch [1/10], Loss: 655.387634\n",
      "Epoch [1/10], Loss: 652.269592\n",
      "Epoch [1/10], Loss: 631.449097\n",
      "Epoch [1/10], Loss: 620.398438\n",
      "Epoch [1/10], Loss: 631.449890\n",
      "Epoch [1/10], Loss: 605.593079\n",
      "Epoch [1/10], Loss: 642.453430\n",
      "Epoch [1/10], Loss: 607.075500\n",
      "Epoch [1/10], Loss: 614.531921\n",
      "Epoch [1/10], Loss: 634.072571\n",
      "Epoch [1/10], Loss: 626.531494\n",
      "Epoch [1/10], Loss: 650.002502\n",
      "Epoch [1/10], Loss: 624.267334\n",
      "Epoch [1/10], Loss: 651.533142\n",
      "Epoch [1/10], Loss: 631.928894\n",
      "Epoch [1/10], Loss: 635.112488\n",
      "Epoch [1/10], Loss: 643.168213\n",
      "Epoch [1/10], Loss: 623.814514\n",
      "Epoch [1/10], Loss: 639.564331\n",
      "Epoch [1/10], Loss: 621.912659\n",
      "Epoch [1/10], Loss: 621.731567\n",
      "Epoch [1/10], Loss: 611.413391\n",
      "Epoch [1/10], Loss: 662.060852\n",
      "Epoch [1/10], Loss: 649.746155\n",
      "Epoch [1/10], Loss: 663.464417\n",
      "Epoch [1/10], Loss: 654.010193\n",
      "Epoch [1/10], Loss: 719.689148\n",
      "Epoch [1/10], Loss: 620.535767\n",
      "Epoch [1/10], Loss: 635.627808\n",
      "Epoch [1/10], Loss: 632.646118\n",
      "Epoch [1/10], Loss: 651.154419\n",
      "Epoch [1/10], Loss: 620.881714\n",
      "Epoch [1/10], Loss: 629.169922\n",
      "Epoch [1/10], Loss: 653.003296\n",
      "Epoch [1/10], Loss: 681.539551\n",
      "Epoch [1/10], Loss: 613.647034\n",
      "Epoch [1/10], Loss: 627.015198\n",
      "Epoch [1/10], Loss: 615.302673\n",
      "Epoch [1/10], Loss: 628.220947\n",
      "Epoch [1/10], Loss: 604.798767\n",
      "Epoch [1/10], Loss: 645.578857\n",
      "Epoch [1/10], Loss: 647.781311\n",
      "Epoch [1/10], Loss: 669.531677\n",
      "Epoch [1/10], Loss: 627.280334\n",
      "Epoch [1/10], Loss: 640.192139\n",
      "Epoch [1/10], Loss: 609.747681\n",
      "Epoch [1/10], Loss: 671.714783\n",
      "Epoch [1/10], Loss: 619.119751\n",
      "Epoch [1/10], Loss: 613.847656\n",
      "Epoch [1/10], Loss: 630.650696\n",
      "Epoch [1/10], Loss: 662.552917\n",
      "Epoch [1/10], Loss: 621.791687\n",
      "Epoch [1/10], Loss: 644.542908\n",
      "Epoch [1/10], Loss: 619.862610\n",
      "Epoch [1/10], Loss: 609.488342\n",
      "Epoch [1/10], Loss: 676.355774\n",
      "Epoch [1/10], Loss: 644.011719\n",
      "Epoch [1/10], Loss: 617.173279\n",
      "Epoch [1/10], Loss: 615.355225\n",
      "Epoch [1/10], Loss: 650.762634\n",
      "Epoch [1/10], Loss: 663.903320\n",
      "Epoch [1/10], Loss: 662.963013\n",
      "Epoch [1/10], Loss: 642.856323\n",
      "Epoch [1/10], Loss: 661.545410\n",
      "Epoch [1/10], Loss: 613.784546\n",
      "Epoch [1/10], Loss: 679.436340\n",
      "Epoch [1/10], Loss: 674.985107\n",
      "Epoch [1/10], Loss: 616.836731\n",
      "Epoch [1/10], Loss: 646.864258\n",
      "Epoch [1/10], Loss: 603.202271\n",
      "Epoch [1/10], Loss: 655.289001\n",
      "Epoch [1/10], Loss: 615.071167\n",
      "Epoch [1/10], Loss: 630.799438\n",
      "Epoch [1/10], Loss: 611.986938\n",
      "Epoch [1/10], Loss: 620.968811\n",
      "Epoch [1/10], Loss: 608.790100\n",
      "Epoch [1/10], Loss: 619.261414\n",
      "Epoch [1/10], Loss: 655.588867\n",
      "Epoch [1/10], Loss: 597.388916\n",
      "Epoch [1/10], Loss: 622.082336\n",
      "Epoch [1/10], Loss: 638.574829\n",
      "Epoch [1/10], Loss: 660.195190\n",
      "Epoch [1/10], Loss: 636.903992\n",
      "Epoch [1/10], Loss: 620.716919\n",
      "Epoch [1/10], Loss: 686.396423\n",
      "Epoch [1/10], Loss: 609.910400\n",
      "Epoch [1/10], Loss: 603.976685\n",
      "Epoch [1/10], Loss: 676.180664\n",
      "Epoch [1/10], Loss: 627.428284\n",
      "Epoch [1/10], Loss: 649.132141\n",
      "Epoch [1/10], Loss: 653.835388\n",
      "Epoch [1/10], Loss: 612.049805\n",
      "Epoch [1/10], Loss: 649.060364\n",
      "Epoch [1/10], Loss: 606.008972\n",
      "Epoch [1/10], Loss: 665.915649\n",
      "Epoch [1/10], Loss: 608.990356\n",
      "Epoch [1/10], Loss: 644.357300\n",
      "Epoch [1/10], Loss: 623.311768\n",
      "Epoch [1/10], Loss: 657.542969\n",
      "Epoch [1/10], Loss: 609.480225\n",
      "Epoch [1/10], Loss: 659.163330\n",
      "Epoch [1/10], Loss: 647.425354\n",
      "Epoch [1/10], Loss: 620.851318\n",
      "Epoch [1/10], Loss: 592.112854\n",
      "Epoch [1/10], Loss: 672.560791\n",
      "Epoch [1/10], Loss: 675.510376\n",
      "Epoch [1/10], Loss: 648.588501\n",
      "Epoch [1/10], Loss: 621.639404\n",
      "Epoch [1/10], Loss: 635.658203\n",
      "Epoch [1/10], Loss: 605.325256\n",
      "Epoch [1/10], Loss: 602.713074\n",
      "Epoch [1/10], Loss: 590.618286\n",
      "Epoch [1/10], Loss: 618.308228\n",
      "Epoch [1/10], Loss: 608.629028\n",
      "Epoch [1/10], Loss: 629.257996\n",
      "Epoch [1/10], Loss: 635.445801\n",
      "Epoch [1/10], Loss: 608.506714\n",
      "Epoch [1/10], Loss: 647.092590\n",
      "Epoch [1/10], Loss: 591.756226\n",
      "Epoch [1/10], Loss: 615.486389\n",
      "Epoch [1/10], Loss: 644.690125\n",
      "Epoch [1/10], Loss: 631.577454\n",
      "Epoch [1/10], Loss: 661.146118\n",
      "Epoch [1/10], Loss: 617.247864\n",
      "Epoch [1/10], Loss: 636.965271\n",
      "Epoch [1/10], Loss: 601.128418\n",
      "Epoch [1/10], Loss: 633.981079\n",
      "Epoch [1/10], Loss: 599.239807\n",
      "Epoch [1/10], Loss: 598.164978\n",
      "Epoch [1/10], Loss: 631.551575\n",
      "Epoch [1/10], Loss: 706.411133\n",
      "Epoch [1/10], Loss: 663.758240\n",
      "Epoch [1/10], Loss: 601.227478\n",
      "Epoch [1/10], Loss: 655.477173\n",
      "Epoch [1/10], Loss: 645.716553\n",
      "Epoch [1/10], Loss: 646.454407\n",
      "Epoch [1/10], Loss: 660.464417\n",
      "Epoch [1/10], Loss: 637.876404\n",
      "Epoch [1/10], Loss: 634.703491\n",
      "Epoch [1/10], Loss: 623.684204\n",
      "Epoch [1/10], Loss: 607.486755\n",
      "Epoch [1/10], Loss: 673.181763\n",
      "Epoch [1/10], Loss: 637.155762\n",
      "Epoch [1/10], Loss: 619.076904\n",
      "Epoch [1/10], Loss: 661.113159\n",
      "Epoch [1/10], Loss: 637.405396\n",
      "Epoch [1/10], Loss: 630.943298\n",
      "Epoch [1/10], Loss: 656.247864\n",
      "Epoch [1/10], Loss: 659.238525\n",
      "Epoch [1/10], Loss: 650.081482\n",
      "Epoch [1/10], Loss: 613.904724\n",
      "Epoch [1/10], Loss: 622.061035\n",
      "Epoch [1/10], Loss: 673.108643\n",
      "Epoch [1/10], Loss: 616.788025\n",
      "Epoch [1/10], Loss: 637.370850\n",
      "Epoch [1/10], Loss: 632.487610\n",
      "Epoch [1/10], Loss: 643.839722\n",
      "Epoch [1/10], Loss: 623.090027\n",
      "Epoch [1/10], Loss: 663.922974\n",
      "Epoch [1/10], Loss: 650.433716\n",
      "Epoch [1/10], Loss: 633.293091\n",
      "Epoch [1/10], Loss: 632.014771\n",
      "Epoch [1/10], Loss: 622.721863\n",
      "Epoch [1/10], Loss: 616.076782\n",
      "Epoch [1/10], Loss: 634.862061\n",
      "Epoch [1/10], Loss: 645.961060\n",
      "Epoch [1/10], Loss: 647.958923\n",
      "Epoch [1/10], Loss: 629.967407\n",
      "Epoch [1/10], Loss: 633.709656\n",
      "Epoch [1/10], Loss: 634.833069\n",
      "Epoch [1/10], Loss: 641.319458\n",
      "Epoch [1/10], Loss: 611.666809\n",
      "Epoch [1/10], Loss: 626.421997\n",
      "Epoch [1/10], Loss: 613.953857\n",
      "Epoch [1/10], Loss: 634.757019\n",
      "Epoch [1/10], Loss: 659.616516\n",
      "Epoch [1/10], Loss: 626.415649\n",
      "Epoch [1/10], Loss: 590.913147\n",
      "Epoch [1/10], Loss: 611.366516\n",
      "Epoch [1/10], Loss: 612.335266\n",
      "Epoch [1/10], Loss: 661.212097\n",
      "Epoch [1/10], Loss: 610.597717\n",
      "Epoch [1/10], Loss: 627.715698\n",
      "Epoch [1/10], Loss: 631.517151\n",
      "Epoch [1/10], Loss: 620.356995\n",
      "Epoch [1/10], Loss: 618.358643\n",
      "Epoch [1/10], Loss: 670.206604\n",
      "Epoch [1/10], Loss: 647.136536\n",
      "Epoch [1/10], Loss: 624.272644\n",
      "Epoch [1/10], Loss: 609.026611\n",
      "Epoch [1/10], Loss: 624.890259\n",
      "Epoch [1/10], Loss: 619.007751\n",
      "Epoch [1/10], Loss: 627.530579\n",
      "Epoch [1/10], Loss: 613.924683\n",
      "Epoch [1/10], Loss: 651.466797\n",
      "Epoch [1/10], Loss: 615.376892\n",
      "Epoch [1/10], Loss: 620.670715\n",
      "Epoch [1/10], Loss: 616.489929\n",
      "Epoch [1/10], Loss: 635.618530\n",
      "Epoch [1/10], Loss: 610.242126\n",
      "Epoch [1/10], Loss: 651.308472\n",
      "Epoch [1/10], Loss: 605.481384\n",
      "Epoch [1/10], Loss: 603.080322\n",
      "Epoch [1/10], Loss: 599.975159\n",
      "Epoch [1/10], Loss: 643.163818\n",
      "Epoch [1/10], Loss: 593.551270\n",
      "Epoch [1/10], Loss: 629.658325\n",
      "Epoch [1/10], Loss: 607.612915\n",
      "Epoch [1/10], Loss: 635.456421\n",
      "Epoch [1/10], Loss: 617.024780\n",
      "Epoch [1/10], Loss: 656.983521\n",
      "Epoch [1/10], Loss: 602.152893\n",
      "Epoch [1/10], Loss: 674.109070\n",
      "Epoch [1/10], Loss: 646.512451\n",
      "Epoch [1/10], Loss: 616.258362\n",
      "Epoch [1/10], Loss: 641.925964\n",
      "Epoch [1/10], Loss: 633.754944\n",
      "Epoch [1/10], Loss: 622.870422\n",
      "Epoch [1/10], Loss: 640.809998\n",
      "Epoch [1/10], Loss: 613.972229\n",
      "Epoch [1/10], Loss: 619.268066\n",
      "Epoch [1/10], Loss: 707.745544\n",
      "Epoch [1/10], Loss: 658.940369\n",
      "Epoch [1/10], Loss: 612.699341\n",
      "Epoch [1/10], Loss: 666.602844\n",
      "Epoch [1/10], Loss: 597.609375\n",
      "Epoch [1/10], Loss: 633.188599\n",
      "Epoch [1/10], Loss: 584.525391\n",
      "Epoch [1/10], Loss: 633.069641\n",
      "Epoch [1/10], Loss: 583.710327\n",
      "Epoch [1/10], Loss: 603.261780\n",
      "Epoch [1/10], Loss: 569.277832\n",
      "Epoch [1/10], Loss: 707.689575\n",
      "Epoch [1/10], Loss: 617.466187\n",
      "Epoch [1/10], Loss: 630.407654\n",
      "Epoch [1/10], Loss: 646.496948\n",
      "Epoch [1/10], Loss: 611.712769\n",
      "Epoch [1/10], Loss: 598.609009\n",
      "Epoch [1/10], Loss: 634.129395\n",
      "Epoch [1/10], Loss: 671.705322\n",
      "Epoch [1/10], Loss: 642.422729\n",
      "Epoch [1/10], Loss: 614.698914\n",
      "Epoch [1/10], Loss: 594.690063\n",
      "Epoch [1/10], Loss: 602.764832\n",
      "Epoch [1/10], Loss: 644.537354\n",
      "Epoch [1/10], Loss: 609.963440\n",
      "Epoch [1/10], Loss: 618.716248\n",
      "Epoch [1/10], Loss: 652.965881\n",
      "Epoch [1/10], Loss: 647.232483\n",
      "Epoch [1/10], Loss: 639.115173\n",
      "Epoch [1/10], Loss: 652.146423\n",
      "Epoch [1/10], Loss: 634.525024\n",
      "Epoch [1/10], Loss: 617.683716\n",
      "Epoch [1/10], Loss: 663.268982\n",
      "Epoch [1/10], Loss: 666.011658\n",
      "Epoch [1/10], Loss: 624.489197\n",
      "Epoch [1/10], Loss: 642.873169\n",
      "Epoch [1/10], Loss: 650.291870\n",
      "Epoch [1/10], Loss: 627.381958\n",
      "Epoch [1/10], Loss: 655.287476\n",
      "Epoch [1/10], Loss: 662.954224\n",
      "Epoch [1/10], Loss: 627.632324\n",
      "Epoch [1/10], Loss: 615.076965\n",
      "Epoch [1/10], Loss: 673.473328\n",
      "Epoch [1/10], Loss: 609.511353\n",
      "Epoch [1/10], Loss: 629.016846\n",
      "Epoch [1/10], Loss: 631.487610\n",
      "Epoch [1/10], Loss: 599.995850\n",
      "Epoch [1/10], Loss: 604.633118\n",
      "Epoch [1/10], Loss: 616.060242\n",
      "Epoch [1/10], Loss: 628.068848\n",
      "Epoch [1/10], Loss: 649.120239\n",
      "Epoch [1/10], Loss: 595.084839\n",
      "Epoch [1/10], Loss: 624.463318\n",
      "Epoch [1/10], Loss: 623.346985\n",
      "Epoch [1/10], Loss: 586.326599\n",
      "Epoch [1/10], Loss: 579.676941\n",
      "Epoch [1/10], Loss: 659.064941\n",
      "Epoch [1/10], Loss: 612.774841\n",
      "Epoch [1/10], Loss: 661.817078\n",
      "Epoch [1/10], Loss: 613.947754\n",
      "Epoch [1/10], Loss: 629.554382\n",
      "Epoch [1/10], Loss: 640.131287\n",
      "Epoch [1/10], Loss: 652.771606\n",
      "Epoch [1/10], Loss: 646.663452\n",
      "Epoch [1/10], Loss: 624.644958\n",
      "Epoch [1/10], Loss: 674.193970\n",
      "Epoch [1/10], Loss: 633.440186\n",
      "Epoch [1/10], Loss: 639.690674\n",
      "Epoch [1/10], Loss: 647.152771\n",
      "Epoch [1/10], Loss: 626.730713\n",
      "Epoch [1/10], Loss: 632.295776\n",
      "Epoch [1/10], Loss: 638.008484\n",
      "Epoch [1/10], Loss: 656.561768\n",
      "Epoch [1/10], Loss: 628.109863\n",
      "Epoch [1/10], Loss: 620.993835\n",
      "Epoch [1/10], Loss: 637.718933\n",
      "Epoch [1/10], Loss: 605.576294\n",
      "Epoch [1/10], Loss: 613.719177\n",
      "Epoch [1/10], Loss: 657.712891\n",
      "Epoch [1/10], Loss: 639.998474\n",
      "Epoch [1/10], Loss: 642.328674\n",
      "Epoch [1/10], Loss: 607.454590\n",
      "Epoch [1/10], Loss: 606.293762\n",
      "Epoch [1/10], Loss: 619.876221\n",
      "Epoch [1/10], Loss: 648.688293\n",
      "Epoch [1/10], Loss: 674.579834\n",
      "Epoch [1/10], Loss: 606.394409\n",
      "Epoch [1/10], Loss: 613.555359\n",
      "Epoch [1/10], Loss: 689.699768\n",
      "Epoch [1/10], Loss: 604.676147\n",
      "Epoch [1/10], Loss: 608.189270\n",
      "Epoch [1/10], Loss: 621.386475\n",
      "Epoch [1/10], Loss: 620.949341\n",
      "Epoch [1/10], Loss: 620.015381\n",
      "Epoch [1/10], Loss: 615.577393\n",
      "Epoch [1/10], Loss: 627.295715\n",
      "Epoch [1/10], Loss: 644.136047\n",
      "Epoch [1/10], Loss: 650.986145\n",
      "Epoch [1/10], Loss: 604.774902\n",
      "Epoch [1/10], Loss: 637.210999\n",
      "Epoch [1/10], Loss: 612.772217\n",
      "Epoch [1/10], Loss: 597.385742\n",
      "Epoch [1/10], Loss: 617.781311\n",
      "Epoch [1/10], Loss: 639.279541\n",
      "Epoch [1/10], Loss: 631.642517\n",
      "Epoch [1/10], Loss: 656.733276\n",
      "Epoch [1/10], Loss: 614.174500\n",
      "Epoch [1/10], Loss: 610.311462\n",
      "Epoch [1/10], Loss: 639.617554\n",
      "Epoch [1/10], Loss: 611.104126\n",
      "Epoch [1/10], Loss: 615.064941\n",
      "Epoch [1/10], Loss: 629.008789\n",
      "Epoch [1/10], Loss: 624.908325\n",
      "Epoch [1/10], Loss: 612.336670\n",
      "Epoch [1/10], Loss: 615.299561\n",
      "Epoch [1/10], Loss: 606.627502\n",
      "Epoch [1/10], Loss: 655.048035\n",
      "Epoch [1/10], Loss: 591.349976\n",
      "Epoch [1/10], Loss: 665.272644\n",
      "Epoch [1/10], Loss: 672.882996\n",
      "Epoch [1/10], Loss: 602.449951\n",
      "Epoch [1/10], Loss: 634.118347\n",
      "Epoch [1/10], Loss: 639.004150\n",
      "Epoch [1/10], Loss: 674.369873\n",
      "Epoch [1/10], Loss: 651.873413\n",
      "Epoch [1/10], Loss: 648.575806\n",
      "Epoch [1/10], Loss: 627.428833\n",
      "Epoch [1/10], Loss: 626.639221\n",
      "Epoch [1/10], Loss: 651.488403\n",
      "Epoch [1/10], Loss: 638.576721\n",
      "Epoch [1/10], Loss: 637.304077\n",
      "Epoch [1/10], Loss: 629.683777\n",
      "Epoch [1/10], Loss: 684.765198\n",
      "Epoch [1/10], Loss: 600.368408\n",
      "Epoch [1/10], Loss: 633.745850\n",
      "Epoch [1/10], Loss: 601.446716\n",
      "Epoch [1/10], Loss: 649.995728\n",
      "Epoch [1/10], Loss: 630.723145\n",
      "Epoch [1/10], Loss: 647.766174\n",
      "Epoch [1/10], Loss: 628.818237\n",
      "Epoch [1/10], Loss: 633.448242\n",
      "Epoch [1/10], Loss: 644.905457\n",
      "Epoch [1/10], Loss: 638.520752\n",
      "Epoch [1/10], Loss: 640.672058\n",
      "Epoch [1/10], Loss: 609.150879\n",
      "Epoch [1/10], Loss: 616.048645\n",
      "Epoch [1/10], Loss: 675.548523\n",
      "Epoch [1/10], Loss: 616.905396\n",
      "Epoch [1/10], Loss: 602.739014\n",
      "Epoch [1/10], Loss: 612.605530\n",
      "Epoch [1/10], Loss: 625.076294\n",
      "Epoch [1/10], Loss: 612.426575\n",
      "Epoch [1/10], Loss: 648.963318\n",
      "Epoch [1/10], Loss: 627.101562\n",
      "Epoch [1/10], Loss: 603.599365\n",
      "Epoch [1/10], Loss: 642.695984\n",
      "Epoch [1/10], Loss: 626.337891\n",
      "Epoch [1/10], Loss: 573.151428\n",
      "Epoch [1/10], Loss: 621.945862\n",
      "Epoch [1/10], Loss: 615.141479\n",
      "Epoch [1/10], Loss: 628.765686\n",
      "Epoch [1/10], Loss: 642.103027\n",
      "Epoch [1/10], Loss: 630.075256\n",
      "Epoch [1/10], Loss: 618.865723\n",
      "Epoch [1/10], Loss: 597.468140\n",
      "Epoch [1/10], Loss: 616.380371\n",
      "Epoch [1/10], Loss: 655.050537\n",
      "Epoch [1/10], Loss: 637.708984\n",
      "Epoch [1/10], Loss: 613.408569\n",
      "Epoch [1/10], Loss: 610.945068\n",
      "Epoch [1/10], Loss: 670.123413\n",
      "Epoch [1/10], Loss: 586.732666\n",
      "Epoch [1/10], Loss: 619.973022\n",
      "Epoch [1/10], Loss: 622.908264\n",
      "Epoch [1/10], Loss: 605.190979\n",
      "Epoch [1/10], Loss: 627.078064\n",
      "Epoch [1/10], Loss: 643.860352\n",
      "Epoch [1/10], Loss: 643.880859\n",
      "Epoch [1/10], Loss: 695.165894\n",
      "Epoch [1/10], Loss: 613.613953\n",
      "Epoch [1/10], Loss: 621.578064\n",
      "Epoch [1/10], Loss: 592.281372\n",
      "Epoch [1/10], Loss: 630.025146\n",
      "Epoch [1/10], Loss: 625.859497\n",
      "Epoch [1/10], Loss: 612.238525\n",
      "Epoch [1/10], Loss: 595.237671\n",
      "Epoch [1/10], Loss: 603.304443\n",
      "Epoch [1/10], Loss: 623.085632\n",
      "Epoch [1/10], Loss: 630.136780\n",
      "Epoch [1/10], Loss: 636.088745\n",
      "Epoch [1/10], Loss: 624.293762\n",
      "Epoch [1/10], Loss: 603.699890\n",
      "Epoch [1/10], Loss: 642.640503\n",
      "Epoch [1/10], Loss: 663.812561\n",
      "Epoch [1/10], Loss: 648.319214\n",
      "Epoch [1/10], Loss: 650.353027\n",
      "Epoch [1/10], Loss: 612.174011\n",
      "Epoch [1/10], Loss: 601.983215\n",
      "Epoch [1/10], Loss: 628.481567\n",
      "Epoch [1/10], Loss: 612.090515\n",
      "Epoch [1/10], Loss: 624.781128\n",
      "Epoch [1/10], Loss: 652.233459\n",
      "Epoch [1/10], Loss: 631.854980\n",
      "Epoch [1/10], Loss: 647.891357\n",
      "Epoch [1/10], Loss: 605.695984\n",
      "Epoch [1/10], Loss: 625.426514\n",
      "Epoch [1/10], Loss: 587.717468\n",
      "Epoch [1/10], Loss: 621.009399\n",
      "Epoch [1/10], Loss: 618.040100\n",
      "Epoch [1/10], Loss: 619.842834\n",
      "Epoch [1/10], Loss: 606.878784\n",
      "Epoch [1/10], Loss: 620.284546\n",
      "Epoch [1/10], Loss: 684.605530\n",
      "Epoch [1/10], Loss: 620.087463\n",
      "Epoch [1/10], Loss: 671.820312\n",
      "Epoch [1/10], Loss: 602.556824\n",
      "Epoch [1/10], Loss: 606.971680\n",
      "Epoch [1/10], Loss: 600.494202\n",
      "Epoch [1/10], Loss: 621.985718\n",
      "Epoch [1/10], Loss: 639.425903\n",
      "Epoch [1/10], Loss: 631.324585\n",
      "Epoch [1/10], Loss: 655.740845\n",
      "Epoch [1/10], Loss: 619.594910\n",
      "Epoch [1/10], Loss: 648.921753\n",
      "Epoch [1/10], Loss: 613.512329\n",
      "Epoch [1/10], Loss: 617.891907\n",
      "Epoch [1/10], Loss: 614.293701\n",
      "Epoch [1/10], Loss: 645.193726\n",
      "Epoch [1/10], Loss: 686.147827\n",
      "Epoch [1/10], Loss: 651.165466\n",
      "Epoch [1/10], Loss: 640.847107\n",
      "Epoch [1/10], Loss: 634.769897\n",
      "Epoch [1/10], Loss: 617.785950\n",
      "Epoch [1/10], Loss: 629.482422\n",
      "Epoch [1/10], Loss: 634.825195\n",
      "Epoch [1/10], Loss: 645.251465\n",
      "Epoch [1/10], Loss: 608.605286\n",
      "Epoch [1/10], Loss: 643.859802\n",
      "Epoch [1/10], Loss: 641.950073\n",
      "Epoch [1/10], Loss: 627.584839\n",
      "Epoch [1/10], Loss: 664.189453\n",
      "Epoch [1/10], Loss: 655.835022\n",
      "Epoch [1/10], Loss: 647.242432\n",
      "Epoch [1/10], Loss: 648.005249\n",
      "Epoch [1/10], Loss: 628.462769\n",
      "Epoch [1/10], Loss: 652.639221\n",
      "Epoch [1/10], Loss: 621.914124\n",
      "Epoch [1/10], Loss: 615.440552\n",
      "Epoch [1/10], Loss: 624.966187\n",
      "Epoch [1/10], Loss: 623.942871\n",
      "Epoch [1/10], Loss: 594.020752\n",
      "Epoch [1/10], Loss: 607.519348\n",
      "Epoch [1/10], Loss: 643.238281\n",
      "Epoch [1/10], Loss: 670.500061\n",
      "Epoch [1/10], Loss: 611.679199\n",
      "Epoch [1/10], Loss: 613.476135\n",
      "Epoch [1/10], Loss: 594.268494\n",
      "Epoch [1/10], Loss: 645.869202\n",
      "Epoch [1/10], Loss: 604.259583\n",
      "Epoch [1/10], Loss: 598.141296\n",
      "Epoch [1/10], Loss: 625.093994\n",
      "Epoch [1/10], Loss: 593.700989\n",
      "Epoch [1/10], Loss: 655.481812\n",
      "Epoch [1/10], Loss: 607.635193\n",
      "Epoch [1/10], Loss: 631.140808\n",
      "Epoch [1/10], Loss: 648.814819\n",
      "Epoch [1/10], Loss: 622.001343\n",
      "Epoch [1/10], Loss: 626.430786\n",
      "Epoch [1/10], Loss: 604.741455\n",
      "Epoch [1/10], Loss: 640.830444\n",
      "Epoch [1/10], Loss: 638.349365\n",
      "Epoch [1/10], Loss: 626.940247\n",
      "Epoch [1/10], Loss: 591.814575\n",
      "Epoch [1/10], Loss: 690.449585\n",
      "Epoch [1/10], Loss: 664.128418\n",
      "Epoch [1/10], Loss: 608.068115\n",
      "Epoch [1/10], Loss: 601.761230\n",
      "Epoch [1/10], Loss: 650.436890\n",
      "Epoch [1/10], Loss: 609.169128\n",
      "Epoch [1/10], Loss: 593.785034\n",
      "Epoch [1/10], Loss: 630.848328\n",
      "Epoch [1/10], Loss: 650.342285\n",
      "Epoch [1/10], Loss: 604.514893\n",
      "Epoch [1/10], Loss: 650.589355\n",
      "Epoch [1/10], Loss: 601.784119\n",
      "Epoch [1/10], Loss: 662.562378\n",
      "Epoch [1/10], Loss: 660.498779\n",
      "Epoch [1/10], Loss: 656.551758\n",
      "Epoch [1/10], Loss: 648.248718\n",
      "Epoch [1/10], Loss: 625.079590\n",
      "Epoch [1/10], Loss: 636.716858\n",
      "Epoch [1/10], Loss: 657.956421\n",
      "Epoch [1/10], Loss: 655.335266\n",
      "Epoch [1/10], Loss: 692.882507\n",
      "Epoch [1/10], Loss: 621.068115\n",
      "Epoch [1/10], Loss: 602.002686\n",
      "Epoch [1/10], Loss: 634.820740\n",
      "Epoch [1/10], Loss: 622.445740\n",
      "Epoch [1/10], Loss: 587.372375\n",
      "Epoch [1/10], Loss: 610.877625\n",
      "Epoch [1/10], Loss: 621.030640\n",
      "Epoch [1/10], Loss: 590.882568\n",
      "Epoch [1/10], Loss: 630.743591\n",
      "Epoch [1/10], Loss: 646.549988\n",
      "Epoch [1/10], Loss: 610.125000\n",
      "Epoch [1/10], Loss: 618.502075\n",
      "Epoch [1/10], Loss: 654.765198\n",
      "Epoch [1/10], Loss: 659.891541\n",
      "Epoch [1/10], Loss: 635.451965\n",
      "Epoch [1/10], Loss: 661.023132\n",
      "Epoch [1/10], Loss: 654.681946\n",
      "Epoch [1/10], Loss: 615.114441\n",
      "Epoch [1/10], Loss: 635.751343\n",
      "Epoch [1/10], Loss: 635.149414\n",
      "Epoch [1/10], Loss: 629.063599\n",
      "Epoch [1/10], Loss: 617.609863\n",
      "Epoch [1/10], Loss: 621.143127\n",
      "Epoch [1/10], Loss: 630.171082\n",
      "Epoch [1/10], Loss: 636.033081\n",
      "Epoch [1/10], Loss: 626.657898\n",
      "Epoch [1/10], Loss: 625.285400\n",
      "Epoch [1/10], Loss: 607.561157\n",
      "Epoch [1/10], Loss: 618.433899\n",
      "Epoch [1/10], Loss: 656.479431\n",
      "Epoch [1/10], Loss: 648.489807\n",
      "Epoch [1/10], Loss: 598.332458\n",
      "Epoch [1/10], Loss: 648.160339\n",
      "Epoch [1/10], Loss: 639.220703\n",
      "Epoch [1/10], Loss: 658.432983\n",
      "Epoch [1/10], Loss: 645.227173\n",
      "Epoch [1/10], Loss: 627.582153\n",
      "Epoch [1/10], Loss: 619.461853\n",
      "Epoch [1/10], Loss: 623.794373\n",
      "Epoch [1/10], Loss: 660.136047\n",
      "Epoch [1/10], Loss: 630.907593\n",
      "Epoch [1/10], Loss: 634.913757\n",
      "Epoch [1/10], Loss: 615.110413\n",
      "Epoch [1/10], Loss: 641.072266\n",
      "Epoch [1/10], Loss: 597.662903\n",
      "Epoch [1/10], Loss: 630.150818\n",
      "Epoch [1/10], Loss: 613.185669\n",
      "Epoch [1/10], Loss: 681.425781\n",
      "Epoch [1/10], Loss: 624.907654\n",
      "Epoch [1/10], Loss: 606.953125\n",
      "Epoch [1/10], Loss: 637.957275\n",
      "Epoch [1/10], Loss: 620.681213\n",
      "Epoch [1/10], Loss: 663.093567\n",
      "Epoch [1/10], Loss: 692.042725\n",
      "Epoch [1/10], Loss: 595.093018\n",
      "Epoch [1/10], Loss: 632.789917\n",
      "Epoch [1/10], Loss: 630.405273\n",
      "Epoch [1/10], Loss: 592.262451\n",
      "Epoch [1/10], Loss: 655.389954\n",
      "Epoch [1/10], Loss: 599.424561\n",
      "Epoch [1/10], Loss: 717.493652\n",
      "Epoch [1/10], Loss: 605.554504\n",
      "Epoch [1/10], Loss: 612.442444\n",
      "Epoch [1/10], Loss: 625.432007\n",
      "Epoch [1/10], Loss: 637.234070\n",
      "Epoch [1/10], Loss: 696.540405\n",
      "Epoch [1/10], Loss: 603.649841\n",
      "Epoch [1/10], Loss: 637.775513\n",
      "Epoch [1/10], Loss: 622.429077\n",
      "Epoch [1/10], Loss: 631.842224\n",
      "Epoch [1/10], Loss: 644.623413\n",
      "Epoch [1/10], Loss: 611.674316\n",
      "Epoch [1/10], Loss: 632.684204\n",
      "Epoch [1/10], Loss: 631.878845\n",
      "Epoch [1/10], Loss: 631.645813\n",
      "Epoch [1/10], Loss: 631.050842\n",
      "Epoch [1/10], Loss: 648.810242\n",
      "Epoch [1/10], Loss: 625.617859\n",
      "Epoch [1/10], Loss: 639.245850\n",
      "Epoch [1/10], Loss: 614.795471\n",
      "Epoch [1/10], Loss: 646.999146\n",
      "Epoch [1/10], Loss: 615.842163\n",
      "Epoch [1/10], Loss: 608.314026\n",
      "Epoch [1/10], Loss: 644.942383\n",
      "Epoch [1/10], Loss: 658.890015\n",
      "Epoch [1/10], Loss: 602.278687\n",
      "Epoch [1/10], Loss: 622.230286\n",
      "Epoch [1/10], Loss: 628.036621\n",
      "Epoch [1/10], Loss: 641.449341\n",
      "Epoch [1/10], Loss: 659.641296\n",
      "Epoch [1/10], Loss: 612.898254\n",
      "Epoch [1/10], Loss: 691.527954\n",
      "Epoch [1/10], Loss: 610.239868\n",
      "Epoch [1/10], Loss: 606.795227\n",
      "Epoch [1/10], Loss: 612.508728\n",
      "Epoch [1/10], Loss: 642.858032\n",
      "Epoch [1/10], Loss: 648.237488\n",
      "Epoch [1/10], Loss: 650.636719\n",
      "Epoch [1/10], Loss: 611.504150\n",
      "Epoch [1/10], Loss: 631.513306\n",
      "Epoch [1/10], Loss: 607.573730\n",
      "Epoch [1/10], Loss: 640.737122\n",
      "Epoch [1/10], Loss: 655.442749\n",
      "Epoch [1/10], Loss: 667.790344\n",
      "Epoch [1/10], Loss: 643.150818\n",
      "Epoch [1/10], Loss: 615.501648\n",
      "Epoch [1/10], Loss: 647.678345\n",
      "Epoch [1/10], Loss: 626.826111\n",
      "Epoch [1/10], Loss: 580.710449\n",
      "Epoch [1/10], Loss: 614.990417\n",
      "Epoch [1/10], Loss: 624.154175\n",
      "Epoch [1/10], Loss: 605.111084\n",
      "Epoch [1/10], Loss: 623.552490\n",
      "Epoch [1/10], Loss: 640.777100\n",
      "Epoch [1/10], Loss: 631.809937\n",
      "Epoch [1/10], Loss: 645.208923\n",
      "Epoch [1/10], Loss: 663.483215\n",
      "Epoch [1/10], Loss: 639.973816\n",
      "Epoch [1/10], Loss: 617.431885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1948542/385336490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mPATH_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Mar5_f_rnn_v18_lr{params['lr_f']}_e{params['num_epochs']}.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mf_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1948542/385336490.py\u001b[0m in \u001b[0;36mtrain_f\u001b[0;34m(point_net, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test-1/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test-1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Diffusion rnn network\n",
    "class DRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DRNet, self).__init__()\n",
    "        \n",
    "        # hidden layer to previous timestep\n",
    "        self.h2h = nn.Sequential(\n",
    "            nn.Linear(256, 784),\n",
    "            nn.BatchNorm1d(784),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 784),\n",
    "            nn.BatchNorm1d(784),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(784, 256),\n",
    "        )\n",
    "\n",
    "    def forward(self, T, g_cumsum, g_pixel_tensor, loss_fn, L):\n",
    "\n",
    "        # t ~ Uniform ({1, ...T})\n",
    "        timestep = torch.arange(1, T+1, 1)\n",
    "        # Initialize y_0 \n",
    "        latent_y_0 = g_cumsum[:, :, T-1]\n",
    "\n",
    "        for t in timestep:\n",
    "            # set eq\n",
    "            a_t = (t+1)/(T+1)\n",
    "            a_t_minus = t/(T+1)\n",
    "            a_T = (T+1)/(T+1)\n",
    "            t_prime = T-t \n",
    "            sigma_t = math.sqrt(a_t*(1-a_t))\n",
    "\n",
    "            # epsilon ~ N(0, I) * 1e-2\n",
    "            epsilon = 1e-2 * torch.randn_like(latent_y_0)\n",
    "            \n",
    "            # locate the current y_t by cumulated sum of g_net output with added noise\n",
    "            y_t = latent_y_0 - a_t/a_T * g_cumsum[:, :, t_prime-1] + sigma_t*epsilon\n",
    "\n",
    "            # calculate expected y_t\n",
    "            expected_y_t_minus = latent_y_0 - a_t_minus/a_T * g_cumsum[:, :, t_prime]\n",
    "\n",
    "            # locate the g(x) at current timepoint\n",
    "            x_point = g_pixel_tensor[:, :, t_prime-1].view(g_pixel_tensor.size(0), -1)\n",
    "            # learn link from sample in the current timepoint sphere to the previous point\n",
    "            input = y_t + x_point\n",
    "            out = self.h2h(input)\n",
    "\n",
    "            # training loss\n",
    "            # lambda*||f, expected_y_(t-1) - (y_t + g(x))||\n",
    "            loss = loss_fn(out, expected_y_t_minus - (y_t + x_point))\n",
    "            lambda_t = 1/a_t_minus - 1/a_t\n",
    "            L += lambda_t * loss\n",
    "        \n",
    "        return out, L\n",
    "\n",
    "# Train function for f\n",
    "def train_f(point_net, params):\n",
    "    num_classes = params['num_classes']\n",
    "    lr = params['lr_f']\n",
    "    num_epochs = params['num_epochs']\n",
    "    T = params['T']\n",
    "    # Initialize network\n",
    "    f_net = DRNet().to(device)\n",
    "    optimizer = torch.optim.AdamW(f_net.parameters(), lr)\n",
    "    loss_fn = torch.nn.MSELoss().to(device)\n",
    "    for e in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            # initialize y_0\n",
    "            y_0 = torch.tensor(F.one_hot(labels, num_classes))\n",
    "            y_0 = y_0.to(torch.float32).to(device)\n",
    "\n",
    "            # # initialize loss\n",
    "            L = loss_fn(y_0, y_0)\n",
    "            L.zero_()\n",
    "            \n",
    "            batch_pc = []\n",
    "            for img in images:\n",
    "                batch_pc.append(img_to_3d(img))\n",
    "            pc = torch.stack(batch_pc, dim=0)\n",
    "            pc = pc.to(torch.float32).to(device)\n",
    "            g_pixel_tensor, feature_transform, tnet_out = point_net(pc)\n",
    "\n",
    "            # take the sum from timestep T to 0, reverse order\n",
    "            g_pixel_tensor = torch.flip(g_pixel_tensor, dims=[2])\n",
    "            g_cumsum = torch.cumsum(g_pixel_tensor, dim=2).to(device)\n",
    "\n",
    "            # train f_net\n",
    "            f_out, L = f_net(T, g_cumsum, g_pixel_tensor, loss_fn, L)\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch [{e+1}/{num_epochs}], Loss: {L.item():.6f}\")\n",
    "            logging.info(f\"Epoch [{e+1}/{num_epochs}], Loss: {L.item():.6f}\")\n",
    "        \n",
    "    return f_net\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # import os\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "    params = {\n",
    "        # hyperparam\n",
    "        \"batch_size\": 64,\n",
    "        \"num_classes\": 10,\n",
    "        \"pixel_count\": 28 * 28,\n",
    "        \"channel_count\": 1,\n",
    "        \"T\": 28 * 28,\n",
    "        \"lr_f\": 0.001,\n",
    "        \"num_epochs\": 10, \n",
    "        \"noise_scale\": 1e-3,\n",
    "        \"point_dimension\": 3\n",
    "    }\n",
    "\n",
    "    # configurate logging function\n",
    "    logging.basicConfig(filename = f\"Mar5_f_rnn_v18_lr{params['lr_f']}_e{params['num_epochs']}_loss.log\",\n",
    "                        level = logging.DEBUG,\n",
    "                        format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "    # load the convolution part of pre-trained encoder\n",
    "    path_g_net = \"point_net_v2.pth\"\n",
    "    g_trained_state_dict = torch.load(path_g_net)\n",
    "    state_dict = {k: v for k, v in g_trained_state_dict.items() if 'base_pointnet' in k}  # Filter to get only 'i2h' parameters\n",
    "    state_dict = {key.replace('base_pointnet.', ''): value for key, value in state_dict.items()}\n",
    "    g_net = BasePointNet(point_dimension=params['point_dimension'])\n",
    "    g_net.load_state_dict(state_dict)\n",
    "    g_net = g_net.to(device)\n",
    "    g_net.eval()\n",
    "    PATH_f = f\"Mar5_f_rnn_v18_lr{params['lr_f']}_e{params['num_epochs']}.pth\"\n",
    "    # save model\n",
    "    f_net = train_f(g_net, params)\n",
    "    torch.save(f_net.state_dict(), PATH_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
