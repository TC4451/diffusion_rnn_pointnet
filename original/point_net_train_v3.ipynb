{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://datascienceub.medium.com/pointnet-implementation-explained-visually-c7e300139698\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# load MNIST dataset, convert to binary pixel values\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Lambda(lambda x: torch.where(x > 0,1,0))\n",
    "                             ]))\n",
    "trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=False)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Lambda(lambda x: torch.where(x > 0,1,0))\n",
    "                             ]))\n",
    "testloader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform image to 3D (x, y, binary value)\n",
    "def img_to_3d(img):\n",
    "    # get coordinates of pixels\n",
    "    coords_x, coords_y = torch.meshgrid(torch.arange(0, img.size(1)), torch.arange(0, img.size(2)))\n",
    "    coords_x = coords_x.flatten().float().unsqueeze(1)\n",
    "    coords_y = coords_y.flatten().float().unsqueeze(1)\n",
    "    values = img.view(-1).unsqueeze(1)\n",
    "    pc = torch.cat((coords_x, coords_y, values), dim=1)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "  \n",
    "class TransformationNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformationNet, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.conv_1 = nn.Conv1d(input_dim, 64, 1)\n",
    "        self.conv_2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv_3 = nn.Conv1d(128, 256, 1)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "        self.bn_2 = nn.BatchNorm1d(128)\n",
    "        self.bn_3 = nn.BatchNorm1d(256)\n",
    "        self.bn_4 = nn.BatchNorm1d(256)\n",
    "        self.bn_5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, self.output_dim*self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_points = x.shape[1]\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "\n",
    "        x = nn.MaxPool1d(num_points)(x)\n",
    "        x = x.view(-1, 256)\n",
    "\n",
    "        x = F.relu(self.bn_4(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_5(self.fc_2(x)))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        identity_matrix = torch.eye(self.output_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            identity_matrix = identity_matrix.cuda()\n",
    "        x = x.view(-1, self.output_dim, self.output_dim) + identity_matrix\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasePointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, point_dimension):\n",
    "        super(BasePointNet, self).__init__()\n",
    "        self.input_transform = TransformationNet(input_dim=point_dimension, output_dim=point_dimension)\n",
    "        self.feature_transform = TransformationNet(input_dim=64, output_dim=64)\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(point_dimension, 64, 1)\n",
    "        self.conv_2 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv_3 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv_4 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv_5 = nn.Conv1d(128, 256, 1)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "        self.bn_2 = nn.BatchNorm1d(64)\n",
    "        self.bn_3 = nn.BatchNorm1d(64)\n",
    "        self.bn_4 = nn.BatchNorm1d(128)\n",
    "        self.bn_5 = nn.BatchNorm1d(256)\n",
    "        \n",
    "\n",
    "    def forward(self, x, plot=False):\n",
    "        num_points = x.shape[1]\n",
    "        \n",
    "        input_transform = self.input_transform(x) # T-Net tensor [batch, 3, 3]\n",
    "        x = torch.bmm(x, input_transform) # Batch matrix-matrix product \n",
    "        x = x.transpose(2, 1) \n",
    "        tnet_out=x.cpu().detach().numpy()\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "\n",
    "        feature_transform = self.feature_transform(x) # T-Net tensor [batch, 64, 64]\n",
    "        x = torch.bmm(x, feature_transform)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "        x = F.relu(self.bn_4(self.conv_4(x)))\n",
    "        x = F.relu(self.bn_5(self.conv_5(x)))\n",
    "        x = torch.sum(x, dim=2)  # summation\n",
    "        x = x.view(-1, 256)  # global feature vector \n",
    "\n",
    "        return x, feature_transform, tnet_out\n",
    "\n",
    "\n",
    "class ClassificationPointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, dropout=0.3, point_dimension=3):\n",
    "        super(ClassificationPointNet, self).__init__()\n",
    "        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, 128)\n",
    "        self.fc_2 = nn.Linear(128, 64)\n",
    "        self.fc_3 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.bn_1 = nn.BatchNorm1d(128)\n",
    "        self.bn_2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, feature_transform, tnet_out = self.base_pointnet(x)\n",
    "\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        x = self.dropout_1(x)\n",
    "\n",
    "        return F.log_softmax(self.fc_3(x), dim=1), feature_transform, tnet_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationPointNet(num_classes=10,\n",
    "                                   point_dimension=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.2997, val loss: 6.870900, train accuracy: 0.9179,  val accuracy: 0.138100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/zilin/miniconda3/envs/test-1/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.2319, val loss: 3.564800, train accuracy: 0.9344,  val accuracy: 0.173800\n",
      "Epoch 2: train loss: 0.2004, val loss: 1.763600, train accuracy: 0.942,  val accuracy: 0.526800\n",
      "Epoch 3: train loss: 0.1846, val loss: 2.333900, train accuracy: 0.9477,  val accuracy: 0.428700\n",
      "Epoch 4: train loss: 0.1606, val loss: 4.095500, train accuracy: 0.9531,  val accuracy: 0.251200\n",
      "Epoch 5: train loss: 0.1441, val loss: 3.404600, train accuracy: 0.9584,  val accuracy: 0.348200\n",
      "Epoch 6: train loss: 0.1353, val loss: 2.605400, train accuracy: 0.9607,  val accuracy: 0.391900\n",
      "Epoch 7: train loss: 0.1231, val loss: 11.817000, train accuracy: 0.9631,  val accuracy: 0.154700\n",
      "Epoch 8: train loss: 0.1131, val loss: 7.339400, train accuracy: 0.9667,  val accuracy: 0.282600\n",
      "Epoch 9: train loss: 0.104, val loss: 2.375800, train accuracy: 0.9692,  val accuracy: 0.424900\n",
      "Epoch 10: train loss: 0.0972, val loss: 5.196700, train accuracy: 0.97,  val accuracy: 0.289600\n",
      "Epoch 11: train loss: 0.0892, val loss: 5.593600, train accuracy: 0.9727,  val accuracy: 0.348100\n",
      "Epoch 12: train loss: 0.1027, val loss: 1.090900, train accuracy: 0.9687,  val accuracy: 0.749900\n",
      "Epoch 13: train loss: 0.0807, val loss: 7.566400, train accuracy: 0.9751,  val accuracy: 0.182600\n",
      "Epoch 14: train loss: 0.075, val loss: 0.696100, train accuracy: 0.9771,  val accuracy: 0.811600\n",
      "Epoch 15: train loss: 0.0733, val loss: 4.939200, train accuracy: 0.9774,  val accuracy: 0.381400\n",
      "Epoch 16: train loss: 0.069, val loss: 4.394200, train accuracy: 0.979,  val accuracy: 0.420100\n",
      "Epoch 17: train loss: 0.064, val loss: 4.122500, train accuracy: 0.9804,  val accuracy: 0.369700\n",
      "Epoch 18: train loss: 0.0593, val loss: 1.339500, train accuracy: 0.9818,  val accuracy: 0.682000\n",
      "Epoch 19: train loss: 0.0547, val loss: 1.836100, train accuracy: 0.9832,  val accuracy: 0.598800\n",
      "Epoch 20: train loss: 0.0515, val loss: 13.545200, train accuracy: 0.9839,  val accuracy: 0.148000\n",
      "Epoch 21: train loss: 0.0461, val loss: 1.636800, train accuracy: 0.9851,  val accuracy: 0.648700\n",
      "Epoch 22: train loss: 0.0481, val loss: 0.456600, train accuracy: 0.9852,  val accuracy: 0.874600\n",
      "Epoch 23: train loss: 0.0435, val loss: 2.004800, train accuracy: 0.9868,  val accuracy: 0.629800\n",
      "Epoch 24: train loss: 0.0408, val loss: 0.891000, train accuracy: 0.9874,  val accuracy: 0.836100\n",
      "Epoch 25: train loss: 0.0378, val loss: 0.287100, train accuracy: 0.9887,  val accuracy: 0.934700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# training model\n",
    "epochs=80\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "best_loss= np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "\n",
    "    # training loop\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        batch_pc = []\n",
    "        for img in images:\n",
    "            batch_pc.append(img_to_3d(img))\n",
    "        pc = torch.stack(batch_pc, dim=0)\n",
    "        pc = pc.to(torch.float32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model = model.train()\n",
    "        preds, feature_transform, tnet_out = model(pc)\n",
    "\n",
    "        identity = torch.eye(feature_transform.shape[-1])\n",
    "        identity = identity.to(device)\n",
    "        regularization_loss = torch.norm(\n",
    "            identity - torch.bmm(feature_transform, feature_transform.transpose(2, 1)))\n",
    "        # Loss\n",
    "        loss = F.nll_loss(preds, labels) + 0.001 * regularization_loss\n",
    "        epoch_train_loss.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = preds.data.max(1)[1]\n",
    "        corrects = preds.eq(labels.data).cpu().sum()\n",
    "\n",
    "        accuracy = corrects.item() / float(images.size(0))\n",
    "        epoch_train_acc.append(accuracy)\n",
    "\n",
    "    epoch_test_loss = []\n",
    "    epoch_test_acc = []\n",
    "\n",
    "    # validation loop\n",
    "    for i, (val_images, val_labels) in enumerate(testloader):\n",
    "        val_batch_pc = []\n",
    "        for img in val_images:\n",
    "            val_batch_pc.append(img_to_3d(img))\n",
    "        val_pc = torch.stack(val_batch_pc, dim=0)\n",
    "        val_pc = val_pc.to(torch.float32).to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "        model = model.eval()\n",
    "        val_preds, feature_transform, tnet_out = model(val_pc)\n",
    "        val_loss = F.nll_loss(val_preds, val_labels)\n",
    "        epoch_test_loss.append(val_loss.cpu().item())\n",
    "        val_preds = val_preds.data.max(1)[1]\n",
    "        corrects = val_preds.eq(val_labels.data).cpu().sum()\n",
    "        accuracy = corrects.item() / float(val_images.size(0))\n",
    "        epoch_test_acc.append(accuracy)\n",
    "\n",
    "    print('Epoch %s: train loss: %s, val loss: %f, train accuracy: %s,  val accuracy: %f'\n",
    "              % (epoch,\n",
    "                round(np.mean(epoch_train_loss), 4),\n",
    "                round(np.mean(epoch_test_loss), 4),\n",
    "                round(np.mean(epoch_train_acc), 4),\n",
    "                round(np.mean(epoch_test_acc), 4)))\n",
    "\n",
    "    if np.mean(test_loss) < best_loss:\n",
    "        state = {\n",
    "            'model':model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict()\n",
    "        }\n",
    "        # torch.save(state, os.path.join('checkpoints', '3Dmnist_checkpoint_%s.pth' % (number_of_points)))\n",
    "        best_loss=np.mean(test_loss)\n",
    "\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    test_loss.append(np.mean(epoch_test_loss))\n",
    "    train_acc.append(np.mean(epoch_train_acc))\n",
    "    test_acc.append(np.mean(epoch_test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_point_net = \"/mnt/VOL1/zilin/diffusion-rnn/code/PointNet/Mar6_point_net_v2_AvgPool.pth\"\n",
    "torch.save(model.state_dict(), path_point_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
